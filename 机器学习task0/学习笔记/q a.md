### 一、讨论前向传播和反向传播的流程？

前向传播是神经网络进行预测的过程。输入的数据与权重相乘再相加，加上偏置（可能没有）后进入激活函数再输出，重复以上过程直到网络最后的输出。

反向传播用于更新网络中的权重和偏置。更新后的参数为原来的参数加上一个$\delta$，$\delta$为学习率乘上梯度（即损失函数对该参数求偏导）的相反数。利用链式法则将梯度展开：损失函数先对激活函数输出求偏导，激活函数输出对激活函数输入求偏导，激活函数输入对参数求偏导。当参数为权重时，最后一项为前一层的激活函数输出；当参数为偏置时，最后一项为1。第一项即为损失函数对后一层每个输入求偏导，后一层每个输入再对激活函数输出求偏导，最后求和。前一层的参数更新需要后一层的数据，所以是反向传播。

### 二、解释梯度下降法，并结合梯度下降法讲讲神经网络工作的流程

梯度下降法是通过参数梯度更新参数优化模型预测的方法。让参数以学习率为步长向梯度下降的方向行进，损失函数就会逐步接近“低谷”，达到局部最小值

神经网络工作的流程：

1. **前向传播**: 首先，通过前向传播计算出神经网络的输出结果。

2. **计算损失函数**: 将网络输出与真实标签进行比较，计算代价函数。

3. **反向传播误差**: 从输出层开始，利用代价函数计算每一层的误差梯度。

4. **更新参数**: 根据反向传播计算得到的梯度更新神经网络中的参数（权重和偏置）。

5. **重复迭代**: 迭代多次以不断优化网络的参数，使得模型在训练数据上表现更好。

### 三、谈谈你对损失函数的理解，并解释交叉熵损失函数的原理

损失函数就是模型得出的预测值相对于真实值的“差别”，不同的损失函数就是用不同的方法去刻画这种“差别”。比如MSE就是用预测值与真实值差的平方的均值来刻画这种“差别”。

交叉熵损失函数：

$$
loss=-\sum_{j=1}^qy_jlog_2\hat{y_j}
$$

是由KL散度（Kullback-Leibler divergence）演化而来，KL散度（以真实值y为基准）:

$$
D_{KL}:=\sum_{i=1}^{n}y_i(f_{\hat{y_i}}(\hat{y_i})-f_{y_i}(y_i))\\
=\sum_{i=1}^{n}y_i((-log_2\hat{y_i})-(-log_2y_i))\\
=\sum_{i=1}^{n}y_i(-log_2\hat{y_i})-H(y_i)
$$

其中$\sum_{i=1}^{n}y_i(-log_2\hat{y_i})$为交叉熵，$H(y_i)$为y的熵，y为真实值熵为0，所以$D_{KL}=\sum_{i=1}^{n}y_i(-log_2\hat{y_i})$，所以叫交叉熵损失函数。

### 四、谈谈你对激活函数的理解

如果没有激活函数或者激活函数为线性函数的话，对前向传播的计算过程进行化简会变为输入入的加权和再加上偏置，即多层网络相当于一层网络，无法到到预期效果。有了非线性的激活函数，网络能拟合更复杂的函数使预测更精确。

### 五、解释梯度消失和梯度爆炸，并给出你觉的有效的解决方法

梯度消失主要出现在深层网络，梯度变的过小导致参数更新缓慢，原因之一是反向传播过程中前面靠近输入的层的梯度计算时，链式法则使用次数多，且数值大多<0，导致梯度愈乘愈小。

梯度爆炸，梯度变的过大甚至出现inf，训练难以进行，原因之一是反向传播过程中前面靠近输入的层的梯度计算时，链式法则使用次数多，且数值大多>0，导致梯度愈乘愈大。

梯度消失有效的解决办法是使用残差网络resnet,引入残差连接后梯度会加上一项，残差连接传递X，梯度为1，即使主干梯度小，整体依然维持较大水准。

梯度爆炸有效的解决办法是归一化，将数据缩放到合理的范围，减轻梯度爆炸。

### 六、解释过拟合，欠拟合，并给出你觉得有效的解决方法

过拟合是较复杂模型“记住”全部的训练数据，表现为训练集精度高，测试集精度低，即训练的模型难以应用到其他数据上。解决方法是简化模型或使用正则化，通过向损失函数添加惩罚项让一些低相关系数起的作用较小，从而简化模型。

欠拟合是由于模型较为简单，无法充分学习数据的特征，导致训练集精度低，测试集精度也低。解决方法是使用更复杂的模型。

### 七、讨论你对正则化（normallize）的认识

正则化能有效防止模型过于复杂导致的过拟合，提高模型泛化能力。常见正则化有三种：L1,L2,L1+L2，L1是在损失函数后加各项参数的绝对值的和乘$\lambda$，L2是在损失函数后加各项参数的平方和乘$\lambda$，L1+L2是在损失函数后加各项参数的绝对值的和乘$\lambda_1$，再加各项参数的平方和乘$\lambda_2$。

### 八、谈谈你对卷积神经网络的理解，并讨论对于cnn具体参数都是什么和如何选择

卷积神经网络是通过卷积核对图片的操作提取特征的深度学习模型。

cnn具体参数：

- 卷积核大小：通常为3x3、5x5，当需要读取较大范围的图片信息时会使用较大卷积核如7x7、11x11

- 卷积核数量：增加卷积核数量能增加通道数，提取更多的特征。一般根据经验选择，如48，128等等

- 步幅和填充：步幅是卷积核每次扫动移动的像素数，一般取1（不跳），2（跳一个像素，使尺寸变小）；填充是在图片周围填上黑色或白色像素，防止尺寸减小过快，一般取1，2

- 池化层：最大池化：取池化窗口内的最大值；平均池化：取池化窗口内的均值。池化窗口大小，一般2x2、3x3；步幅一般为2；一般不进行填充

- 学习率：权重更新的幅度，一般较复杂取小一点，如0.0001，较简单取大一些，如0.01

- 批量大小：将训练集分批输入模型训练，每批数量一般为128，256等

- 训练轮次：训练全部数据的次数，收敛晚增加次数，收敛早减少次数，一般10，20

- 卷积层、全连接层数量：越庞大复杂的数据集用越深的网络

### 九、1*1的卷积有什么作用

1*1即$k_h = k_w = 1$的卷积核，它只扫一个像素，所以不识别空间信息，但多个1x1的卷积可以融合通道

### 十、卷积神经网络中池化的作用

- 减少图片的尺寸，增加计算速度

- 提取主要特征，抛弃不必要的细节，提高精度
